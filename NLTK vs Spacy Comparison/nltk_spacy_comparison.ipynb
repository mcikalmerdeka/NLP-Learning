{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK vs spaCy: Comprehensive Code Comparison\n",
    "\n",
    "This guide offers side-by-side code comparisons between NLTK and spaCy for a wide range of NLP tasks, from basic to advanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#setup-and-installation)\n",
    "2. [Basic NLP Tasks](#basic-nlp-tasks)\n",
    "   - [Tokenization](#1-tokenization)\n",
    "   - [Part-of-Speech Tagging](#2-part-of-speech-tagging)\n",
    "   - [Lemmatization](#3-lemmatization)\n",
    "   - [Stemming](#4-stemming)\n",
    "   - [Stop Words Removal](#5-stop-words-removal)\n",
    "3. [Intermediate NLP Tasks](#intermediate-nlp-tasks)\n",
    "   - [Named Entity Recognition](#6-named-entity-recognition)\n",
    "   - [Dependency Parsing](#7-dependency-parsing)\n",
    "   - [Sentence Segmentation](#8-sentence-segmentation)\n",
    "   - [Text Preprocessing Pipeline](#9-text-preprocessing-pipeline)\n",
    "   - [N-grams Generation](#10-n-grams-generation)\n",
    "4. [Advanced NLP Tasks](#advanced-nlp-tasks)\n",
    "   - [Word Embeddings](#11-word-embeddings)\n",
    "   - [Text Classification](#12-text-classification)\n",
    "   - [Sentiment Analysis](#13-sentiment-analysis)\n",
    "   - [Topic Modeling](#14-topic-modeling)\n",
    "   - [Text Summarization](#15-text-summarization)\n",
    "   - [Language Detection](#16-language-detection)\n",
    "   - [Chunking and Shallow Parsing](#17-chunking-and-shallow-parsing)\n",
    "   - [Custom NLP Pipeline Components](#18-custom-nlp-pipeline-components)\n",
    "5. [Practical Use Cases](#practical-use-cases)\n",
    "   - [Text Similarity Comparison](#19-text-similarity-comparison)\n",
    "   - [Keyword Extraction](#20-keyword-extraction)\n",
    "   - [Question Answering](#21-question-answering)\n",
    "6. [Performance Benchmarks](#performance-benchmarks)\n",
    "7. [When to Use Which Library](#when-to-use-which-library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Let's start with how to set up each library:\n",
    "\n",
    "### Core NLP libraries\n",
    "```python\n",
    "uv add nltk spacy\n",
    "```\n",
    "\n",
    "### Scikit-learn for ML components\n",
    "```python\n",
    "uv add scikit-learn\n",
    "```\n",
    "\n",
    "### Additional processing libraries\n",
    "```python\n",
    "uv add fensim textblob pandas numpy\n",
    "```\n",
    "\n",
    "### Visualization\n",
    "```python\n",
    "uv add matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk and spacy\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download spaCy language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 7.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# small model --> faster\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# # medium model --> included word vectors\n",
    "# python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Cikal Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Cikal Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Cikal\n",
      "[nltk_data]     Merdeka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_comparison(text):\n",
    "    \"\"\"Compare tokenization between NLTK and spaCy\"\"\"\n",
    "\n",
    "    # Sample text\n",
    "    print(f\"Original text: {text}\")\n",
    "\n",
    "    # NLTK tokenization\n",
    "    from nltk import word_tokenize\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    print(f\"\\nNLTK tokens: {nltk_tokens}\")\n",
    "    print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
    "\n",
    "    # spaCy tokenization\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "    print(f\"\\nspaCy tokens: {spacy_tokens}\")\n",
    "    print(f\"Number of tokens: {len(spacy_tokens)}\")\n",
    "    \n",
    "    # Compare tokenization results\n",
    "    print(\"\\nTokenization Comparison:\")\n",
    "    nltk_set = set(nltk_tokens)\n",
    "    spacy_set = set(spacy_tokens)\n",
    "    print(f\"NLTK unique tokens: {nltk_set}\")\n",
    "    print(f\"spaCy unique tokens: {spacy_set}\")\n",
    "    print(f\"\\nTokens in NLTK but not in spaCy: {nltk_set - spacy_set}\")\n",
    "    print(f\"\\nTokens in spaCy but not in NLTK: {spacy_set - nltk_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Mr. Smith paid $30.00 for the U.S.A. trip-package. It's worth it!\n",
      "\n",
      "NLTK tokens: ['Mr.', 'Smith', 'paid', '$', '30.00', 'for', 'the', 'U.S.A.', 'trip-package', '.', 'It', \"'s\", 'worth', 'it', '!']\n",
      "Number of tokens: 15\n",
      "\n",
      "spaCy tokens: ['Mr.', 'Smith', 'paid', '$', '30.00', 'for', 'the', 'U.S.A.', 'trip', '-', 'package', '.', 'It', \"'s\", 'worth', 'it', '!']\n",
      "Number of tokens: 17\n",
      "\n",
      "Tokenization Comparison:\n",
      "NLTK unique tokens: {'worth', '$', '30.00', 'paid', 'for', '!', 'Mr.', \"'s\", 'trip-package', '.', 'the', 'It', 'it', 'U.S.A.', 'Smith'}\n",
      "spaCy unique tokens: {'worth', 'package', '$', '30.00', '-', 'paid', 'for', '!', 'trip', 'Mr.', \"'s\", 'it', '.', 'the', 'It', 'U.S.A.', 'Smith'}\n",
      "\n",
      "Tokens in NLTK but not in spaCy: {'trip-package'}\n",
      "\n",
      "Tokens in spaCy but not in NLTK: {'trip', 'package', '-'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Mr. Smith paid $30.00 for the U.S.A. trip-package. It's worth it!\"\n",
    "tokenize_comparison(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tag Explanation:\n",
    "\n",
    "- **NLTK POS Tags (Penn Treebank tagset):**\n",
    "    - DT: Determiner (e.g., the, a, an, this, that)\n",
    "    - JJ: Adjective (e.g., quick, lazy, beautiful)\n",
    "    - NN: Noun, singular or mass (e.g., fox, dog, house)\n",
    "    - VBZ: Verb, 3rd person singular present (e.g., jumps, runs, eats)\n",
    "    - IN: Preposition or subordinating conjunction (e.g., over, in, on, by)\n",
    "    - VB: Verb, base form (e.g., go, run, eat)\n",
    "    - VBD: Verb, past tense (e.g., went, ran, ate)\n",
    "    - VBG: Verb, gerund or present participle (e.g., going, running, eating)\n",
    "    - VBN: Verb, past participle (e.g., gone, run, eaten)\n",
    "    - VBP: Verb, non-3rd person singular present (e.g., go, run, eat)\n",
    "    - NNS: Noun, plural (e.g., foxes, dogs, houses)\n",
    "    - NNP: Proper noun, singular (e.g., John, London, Microsoft)\n",
    "    - NNPS: Proper noun, plural (e.g., Americans, Romans)\n",
    "    - RB: Adverb (e.g., quickly, silently, well)\n",
    "    - PRP: Personal pronoun (e.g., I, you, he, she, it)\n",
    "    - PRP$: Possessive pronoun (e.g., my, your, his)\n",
    "    - CC: Coordinating conjunction (e.g., and, but, or)\n",
    "    - CD: Cardinal number (e.g., one, two, three)\n",
    "\n",
    "- spaCy POS Tags (Universal Dependencies tagset):\n",
    "    - DET: Determiner (corresponds to DT in Penn Treebank)\n",
    "    - ADJ: Adjective (corresponds to JJ in Penn Treebank)\n",
    "    - NOUN: Noun (corresponds to NN, NNS in Penn Treebank)\n",
    "    - VERB: Verb (corresponds to VB, VBD, VBG, VBN, VBP, VBZ in Penn Treebank)\n",
    "    - ADP: Adposition (preposition or postposition, corresponds to IN in Penn Treebank)\n",
    "    - ADV: Adverb (corresponds to RB in Penn Treebank)\n",
    "    - PRON: Pronoun (corresponds to PRP, PRP$ in Penn Treebank)\n",
    "    - PROPN: Proper noun (corresponds to NNP, NNPS in Penn Treebank)\n",
    "    - CCONJ: Coordinating conjunction (corresponds to CC in Penn Treebank)\n",
    "    - NUM: Numeral (corresponds to CD in Penn Treebank)\n",
    "    - PUNCT: Punctuation\n",
    "    - SYM: Symbol\n",
    "    - X: Other\n",
    "\n",
    "Note: spaCy uses a coarse-grained tag (pos_) and a fine-grained tag (tag_).\n",
    "The fine-grained tags often match the Penn Treebank tags used by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_comparison(text):\n",
    "    \"\"\"Compare part-of speech tagging between NLTK and spaCy\"\"\"\n",
    "\n",
    "    # Sample text\n",
    "    print(f\"Original text: {text}\")\n",
    "\n",
    "    # NLTK POS tagging\n",
    "    from nltk import word_tokenize\n",
    "    from nltk import pos_tag\n",
    "\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    nltk_po_tags = pos_tag(nltk_tokens)\n",
    "\n",
    "    print(\"\\nNLTK POS Tags:\")\n",
    "    for token, tag in nltk_po_tags:\n",
    "        print(f\"{token}: {tag}\")\n",
    "\n",
    "\n",
    "    # spaCy POS tagging\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print(\"\\nspaCy POS Tags:\")\n",
    "    for token in doc:\n",
    "        # Print both simple POS and detailed POS\n",
    "        print(f\"{token.text}: {token.pos_} (fine-grained: {token.tag_})\")\n",
    "\n",
    "    # Note: NLTK uses Penn Treebank tagset, while spaCy uses Universal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumps over the lazy dog\n",
      "\n",
      "NLTK POS Tags:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      "\n",
      "spaCy POS Tags:\n",
      "The: DET (fine-grained: DT)\n",
      "quick: ADJ (fine-grained: JJ)\n",
      "brown: ADJ (fine-grained: JJ)\n",
      "fox: NOUN (fine-grained: NN)\n",
      "jumps: VERB (fine-grained: VBZ)\n",
      "over: ADP (fine-grained: IN)\n",
      "the: DET (fine-grained: DT)\n",
      "lazy: ADJ (fine-grained: JJ)\n",
      "dog: NOUN (fine-grained: NN)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "pos_tagging_comparison(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_comparison(text):\n",
    "    \"\"\"Compare lemmatization between NLTK and spaCy\"\"\"\n",
    "  \n",
    "    print(f\"Original text: {text}\")\n",
    "  \n",
    "    # NLTK Lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk import pos_tag\n",
    "  \n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "    # NLTK requires POS information for better lemmatization\n",
    "    # We need to convert Penn Treebank tags to WordNet tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'  # adjective\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'  # verb\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'  # noun\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'  # adverb\n",
    "        else:\n",
    "            return 'n'  # default to noun\n",
    "  \n",
    "    # Tokenize and get POS tags\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    nltk_pos = pos_tag(nltk_tokens)\n",
    "  \n",
    "    # Lemmatize with POS tags\n",
    "    nltk_lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) \n",
    "                   for word, pos in nltk_pos]\n",
    "  \n",
    "    # Simple lemmatization (without POS, defaults to nouns)\n",
    "    nltk_simple_lemmas = [lemmatizer.lemmatize(word) for word in nltk_tokens]\n",
    "  \n",
    "    print(\"\\nNLTK lemmas (with POS):\")\n",
    "    for original, lemma in zip(nltk_tokens, nltk_lemmas):\n",
    "        print(f\"{original} -> {lemma}\")\n",
    "  \n",
    "    print(\"\\nNLTK simple lemmas (without POS):\")\n",
    "    for original, lemma in zip(nltk_tokens, nltk_simple_lemmas):\n",
    "        print(f\"{original} -> {lemma}\")\n",
    "  \n",
    "    # spaCy Lemmatization\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "  \n",
    "    print(\"\\nspaCy lemmas:\")\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The cats are running and jumping over many boxes.\n",
      "\n",
      "NLTK lemmas (with POS):\n",
      "The -> The\n",
      "cats -> cat\n",
      "are -> be\n",
      "running -> run\n",
      "and -> and\n",
      "jumping -> jump\n",
      "over -> over\n",
      "many -> many\n",
      "boxes -> box\n",
      ". -> .\n",
      "\n",
      "NLTK simple lemmas (without POS):\n",
      "The -> The\n",
      "cats -> cat\n",
      "are -> are\n",
      "running -> running\n",
      "and -> and\n",
      "jumping -> jumping\n",
      "over -> over\n",
      "many -> many\n",
      "boxes -> box\n",
      ". -> .\n",
      "\n",
      "spaCy lemmas:\n",
      "The -> the\n",
      "cats -> cat\n",
      "are -> be\n",
      "running -> run\n",
      "and -> and\n",
      "jumping -> jump\n",
      "over -> over\n",
      "many -> many\n",
      "boxes -> box\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"The cats are running and jumping over many boxes.\"\n",
    "lemmatization_comparison(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
