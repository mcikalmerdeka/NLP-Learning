{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0d03a1",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4977d",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory) Networks Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM output shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size # number of features in the hidden state\n",
    "        self.num_layers = num_layers # number of LSTM layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # batch_first=True means that the first dimension of the input and output tensors is the batch dimension\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Example usage for sequence prediction\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "seq_length = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Generate a sample input tensor\n",
    "sample_input = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Compute the output\n",
    "output = model(sample_input)\n",
    "print(f\"LSTM output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd184219",
   "metadata": {},
   "source": [
    "## NLP Sentiment Analysis Example with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Define the LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    # Set the parameters for the LSTM model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.3):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer to convert word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert word indices to embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        output = self.dropout(last_hidden)\n",
    "        \n",
    "        # Final classification layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664ee1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing utilities\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from training texts\"\"\"\n",
    "        # Tokenize and count words\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Build vocabulary (words that appear at least min_freq times)\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        idx = 2\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_freq:\n",
    "                self.word_to_idx[word] = idx\n",
    "                self.idx_to_word[idx] = word\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple tokenization (can be enhanced with NLTK/spaCy)\"\"\"\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        return text.split()\n",
    "    \n",
    "    def text_to_sequence(self, text, max_length=100):\n",
    "        \"\"\"Convert text to sequence of word indices\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        # Convert words to indices\n",
    "        sequence = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens]\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length]\n",
    "        else:\n",
    "            sequence.extend([self.word_to_idx['<PAD>']] * (max_length - len(sequence)))\n",
    "            \n",
    "        return torch.tensor(sequence, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164bb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 20 samples\n",
      "Positive samples: 10\n",
      "Negative samples: 10\n",
      "Vocabulary size: 86\n",
      "Sample vocabulary: ['<PAD>', '<UNK>', 'i', 'love', 'this', 'movie', 'its', 'absolutely', 'amazing', 'film']\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset for sentiment analysis\n",
    "sample_texts = [\n",
    "    \"I love this movie, it's absolutely amazing!\",\n",
    "    \"This film is terrible, worst I've ever seen.\",\n",
    "    \"Great acting and wonderful storyline.\",\n",
    "    \"Boring and predictable plot.\",\n",
    "    \"Fantastic cinematography and excellent performances!\",\n",
    "    \"Waste of time, very disappointing.\",\n",
    "    \"I really enjoyed watching this.\",\n",
    "    \"Not worth the money, very bad quality.\",\n",
    "    \"Outstanding movie with brilliant direction.\",\n",
    "    \"Poor script and bad acting.\",\n",
    "    \"Incredible story that kept me engaged throughout.\",\n",
    "    \"Completely overrated and boring.\",\n",
    "    \"Beautiful visuals and amazing soundtrack.\",\n",
    "    \"Terrible dialogue and poor character development.\",\n",
    "    \"One of the best films I've ever watched!\",\n",
    "    \"Disappointed with the ending, very confusing.\",\n",
    "    \"Excellent cast and brilliant performances.\",\n",
    "    \"Wasted two hours of my life on this garbage.\",\n",
    "    \"Masterpiece of modern cinema!\",\n",
    "    \"Absolutely horrible, avoid at all costs.\"\n",
    "]\n",
    "\n",
    "# Labels: 1 for positive, 0 for negative\n",
    "sample_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(f\"Dataset size: {len(sample_texts)} samples\")\n",
    "print(f\"Positive samples: {sum(sample_labels)}\")\n",
    "print(f\"Negative samples: {len(sample_labels) - sum(sample_labels)}\")\n",
    "\n",
    "# Initialize preprocessor and build vocabulary\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.build_vocab(sample_texts, min_freq=1)  # Lower threshold for small dataset\n",
    "\n",
    "print(f\"Vocabulary size: {preprocessor.vocab_size}\")\n",
    "print(\"Sample vocabulary:\", list(preprocessor.word_to_idx.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460b9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: torch.Size([20, 20])\n",
      "Labels shape: torch.Size([20])\n",
      "Sample sequence: tensor([2, 3, 4, 5, 6, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Sample text: 'I love this movie, it's absolutely amazing!'\n",
      "\n",
      "Model Architecture:\n",
      "- Vocabulary size: 86\n",
      "- Embedding dimension: 100\n",
      "- Hidden size: 128\n",
      "- Number of layers: 2\n",
      "- Output classes: 2\n",
      "- Total parameters: 258,714\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences and prepare data\n",
    "max_length = 20\n",
    "sequences = torch.stack([preprocessor.text_to_sequence(text, max_length) for text in sample_texts])\n",
    "labels = torch.tensor(sample_labels, dtype=torch.long)\n",
    "\n",
    "print(f\"Sequences shape: {sequences.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Sample sequence: {sequences[0]}\")\n",
    "print(f\"Sample text: '{sample_texts[0]}'\")\n",
    "\n",
    "# Model parameters for sentiment analysis\n",
    "vocab_size = preprocessor.vocab_size\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 2  # binary classification (positive/negative)\n",
    "\n",
    "# Initialize sentiment model\n",
    "sentiment_model = SentimentLSTM(vocab_size, embedding_dim, hidden_size, num_layers, output_size)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"- Vocabulary size: {vocab_size}\")\n",
    "print(f\"- Embedding dimension: {embedding_dim}\")\n",
    "print(f\"- Hidden size: {hidden_size}\")\n",
    "print(f\"- Number of layers: {num_layers}\")\n",
    "print(f\"- Output classes: {output_size}\")\n",
    "print(f\"- Total parameters: {sum(p.numel() for p in sentiment_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d3eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiment model (state dict is preferred for PyTorch models)\n",
    "torch.save(sentiment_model.state_dict(), \"sentiment_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73430e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the sentiment analysis model...\n",
      "==================================================\n",
      "Epoch [50/200], Loss: 0.6615, Accuracy: 0.6000\n",
      "Epoch [100/200], Loss: 0.0025, Accuracy: 1.0000\n",
      "Epoch [150/200], Loss: 0.0003, Accuracy: 1.0000\n",
      "Epoch [200/200], Loss: 0.0002, Accuracy: 1.0000\n",
      "==================================================\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sentiment_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, sequences, labels, epochs=200, print_every=50):\n",
    "    \"\"\"Train the sentiment analysis model\"\"\"\n",
    "    model.train()\n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            # Calculate accuracy\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                accuracy = (predictions == labels).float().mean().item()\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return model, training_losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the sentiment analysis model...\")\n",
    "print(\"=\" * 50)\n",
    "trained_model, losses = train_model(sentiment_model, sequences, labels)\n",
    "print(\"=\" * 50)\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa1e20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Performance:\n",
      "- Overall Accuracy: 1.0000\n",
      "- Positive Class Accuracy: 1.0000\n",
      "- Negative Class Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and prediction functions\n",
    "def predict_sentiment(model, preprocessor, text, max_length=20):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = preprocessor.text_to_sequence(text, max_length).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(sequence)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "        return sentiment, confidence, probabilities[0].tolist()\n",
    "\n",
    "def evaluate_model(model, sequences, labels):\n",
    "    \"\"\"Evaluate model performance on given data\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequences)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        accuracy = (predictions == labels).float().mean().item()\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        positive_correct = ((predictions == 1) & (labels == 1)).sum().item()\n",
    "        positive_total = (labels == 1).sum().item()\n",
    "        negative_correct = ((predictions == 0) & (labels == 0)).sum().item()\n",
    "        negative_total = (labels == 0).sum().item()\n",
    "        \n",
    "        positive_accuracy = positive_correct / positive_total if positive_total > 0 else 0\n",
    "        negative_accuracy = negative_correct / negative_total if negative_total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'overall_accuracy': accuracy,\n",
    "            'positive_accuracy': positive_accuracy,\n",
    "            'negative_accuracy': negative_accuracy,\n",
    "            'predictions': predictions.tolist(),\n",
    "            'probabilities': F.softmax(outputs, dim=1).tolist()\n",
    "        }\n",
    "\n",
    "# Evaluate on training data\n",
    "eval_results = evaluate_model(trained_model, sequences, labels)\n",
    "print(f\"Training Set Performance:\")\n",
    "print(f\"- Overall Accuracy: {eval_results['overall_accuracy']:.4f}\")\n",
    "print(f\"- Positive Class Accuracy: {eval_results['positive_accuracy']:.4f}\")\n",
    "print(f\"- Negative Class Accuracy: {eval_results['negative_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c24e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SENTIMENT ANALYSIS PREDICTIONS\n",
      "======================================================================\n",
      " 1. Text: 'This movie is absolutely fantastic and incredible!'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 2. Text: 'I hate this boring and terrible film.'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 3. Text: 'Not bad, could be better though.'\n",
      "    Prediction: Negative (Confidence: 1.000)\n",
      "    Probabilities: Negative=1.000, Positive=0.000\n",
      "----------------------------------------------------------------------\n",
      " 4. Text: 'Amazing story with great characters and wonderful acting!'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 5. Text: 'The worst movie I have ever seen in my entire life.'\n",
      "    Prediction: Negative (Confidence: 1.000)\n",
      "    Probabilities: Negative=1.000, Positive=0.000\n",
      "----------------------------------------------------------------------\n",
      " 6. Text: 'Pretty good, I enjoyed it overall.'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 7. Text: 'Mediocre film with some decent moments.'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 8. Text: 'Brilliant masterpiece that everyone should watch!'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      " 9. Text: 'Complete garbage, total waste of time.'\n",
      "    Prediction: Negative (Confidence: 1.000)\n",
      "    Probabilities: Negative=1.000, Positive=0.000\n",
      "----------------------------------------------------------------------\n",
      "10. Text: 'An okay movie, nothing special but watchable.'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      "11. Text: 'I'm not sure about this one, it's just okay.'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      "12. Text: 'This movie is a masterpiece, I'm blown away!'\n",
      "    Prediction: Positive (Confidence: 1.000)\n",
      "    Probabilities: Negative=0.000, Positive=1.000\n",
      "----------------------------------------------------------------------\n",
      "13. Text: 'Terrible, I'm so disappointed.'\n",
      "    Prediction: Negative (Confidence: 1.000)\n",
      "    Probabilities: Negative=1.000, Positive=0.000\n",
      "----------------------------------------------------------------------\n",
      "14. Text: 'I'm not sure what to think about this one.'\n",
      "    Prediction: Negative (Confidence: 1.000)\n",
      "    Probabilities: Negative=1.000, Positive=0.000\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model with new examples\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic and incredible!\",\n",
    "    \"I hate this boring and terrible film.\",\n",
    "    \"Not bad, could be better though.\",\n",
    "    \"Amazing story with great characters and wonderful acting!\",\n",
    "    \"The worst movie I have ever seen in my entire life.\",\n",
    "    \"Pretty good, I enjoyed it overall.\",\n",
    "    \"Mediocre film with some decent moments.\",\n",
    "    \"Brilliant masterpiece that everyone should watch!\",\n",
    "    \"Complete garbage, total waste of time.\",\n",
    "    \"An okay movie, nothing special but watchable.\",\n",
    "    \"I'm not sure about this one, it's just okay.\",\n",
    "    \"This movie is a masterpiece, I'm blown away!\",\n",
    "    \"Terrible, I'm so disappointed.\",\n",
    "    \"I'm not sure what to think about this one.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SENTIMENT ANALYSIS PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    sentiment, confidence, probs = predict_sentiment(trained_model, preprocessor, text)\n",
    "    neg_prob, pos_prob = probs\n",
    "    \n",
    "    print(f\"{i:2d}. Text: '{text}'\")\n",
    "    print(f\"    Prediction: {sentiment} (Confidence: {confidence:.3f})\")\n",
    "    print(f\"    Probabilities: Negative={neg_prob:.3f}, Positive={pos_prob:.3f}\")\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c012e8",
   "metadata": {},
   "source": [
    "### Key Improvements for Real-World Applications\n",
    "\n",
    "1. **Larger Dataset**: Use thousands of samples instead of 20\n",
    "2. **Pre-trained Embeddings**: Use GloVe, Word2Vec, or FastText embeddings\n",
    "3. **Advanced Preprocessing**: Use NLTK/spaCy for tokenization, stemming, lemmatization\n",
    "4. **Train/Validation Split**: Proper data splitting for model evaluation\n",
    "5. **Bidirectional LSTM**: Process sequences in both directions\n",
    "6. **Attention Mechanism**: Focus on important words\n",
    "7. **Regularization**: Add more dropout, weight decay, early stopping\n",
    "8. **Hyperparameter Tuning**: Grid search for optimal parameters\n",
    "\n",
    "### Applications of LSTM in NLP:\n",
    "- **Sentiment Analysis**: Customer reviews, social media posts\n",
    "- **Language Modeling**: Next word prediction, text generation\n",
    "- **Machine Translation**: Sequence-to-sequence models\n",
    "- **Named Entity Recognition**: Identifying entities in text\n",
    "- **Text Summarization**: Extracting key information\n",
    "- **Question Answering**: Understanding and responding to queries\n",
    "- **Speech Recognition**: Converting audio to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211628c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
