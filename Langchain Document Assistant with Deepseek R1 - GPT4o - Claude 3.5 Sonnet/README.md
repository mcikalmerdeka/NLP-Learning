# DocuChat AI: Intelligent Document Assistant

![Project Header](./assets/Project%20Header.jpg)

A powerful LLM-powered document assistant that extracts information from various documents, supporting files containing both text and image components. This application enables seamless data retrieval and analysis from your content library with **enhanced external search capabilities** for comprehensive answers.

## üåê Try the Live Demo

**üöÄ [Launch DocuChat AI](https://docu-chat-ai.streamlit.app/)**

Experience the power of AI-driven document analysis instantly! No setup required - just upload your PDF and start asking questions.

### Cloud vs Local Deployment

| Feature | **Cloud Version** (Streamlit Community Cloud) | **Local Version** (`rag_gpt_claude.py`) |
|---------|-----------------------------------------------|------------------------------------------|
| **Setup** | Zero setup - ready to use | Requires local installation & API keys |
| **Storage** | InMemoryVectorStore (session-based) | ChromaDB (persistent) |
| **File Persistence** | Documents reset on app restart | Documents saved permanently |
| **Performance** | Shared resources, 1GB memory limit | Full local resources |
| **Concurrent Users** | 3-5 users on free tier | Single user (your machine) |
| **External Search** | Limited without Tavily API key | Full capability with API keys |
| **Privacy** | Documents processed on Streamlit servers | Complete local privacy |
| **Best For** | Quick testing, sharing, demos | Production use, large documents, privacy |

## üöÄ Features

- **Multiple LLM Support**: Integrates with GPT-4o, GPT-4.1, Claude Sonnet 4, and DeepSeek-R1 models
- **Document Processing**: Efficiently processes PDF documents, extracting and indexing their content
- **Semantic Search**: Uses vector embeddings to find the most relevant information from your documents
- **üîç External Search Integration**: Automatically searches external sources when document context is insufficient
- **Conversational Interface**: Clean, intuitive chat interface to ask questions about your documents
- **Custom Styling**: Dark mode interface with responsive design
- **Multiple Vector Store Options**: Supports both ChromaDB (current approach) and InMemoryVectorStore (legacy approach)
- **Intelligent Search Modes**: Toggle between document-only mode and enhanced mode with external search

## üõ†Ô∏è Implementation Details

- Built with **LangChain** framework for document processing and retrieval augmented generation (RAG)
- Utilizes **Streamlit** for the web interface
- Implements chunking with `RecursiveCharacterTextSplitter` for optimal document segmentation
- **External Search**: Powered by **Tavily Search API** for up-to-date external information
- **Intelligent Agent System**: Uses LangChain agents with ReAct pattern for external resource lookup
- Supports two vector store implementations:
  - **ChromaDB** (current approach): Persistent vector database for better scalability and performance
  - **InMemoryVectorStore** (legacy approach): Simple in-memory storage for lightweight applications
- Supports semantic search with both cloud and local embedding models
- Handles PDF documents using `PyMuPDFLoader` (GPT/Claude version) and `PDFPlumberLoader` (DeepSeek version)
- Includes chat history management for continuous conversations

## üîç External Search Integration

### How It Works

1. **Document Analysis First**: When you ask a question, the AI first tries to answer using your uploaded document
2. **Automatic External Search**: If the document doesn't contain sufficient information, the AI automatically searches external sources via Tavily
3. **Combined Response**: You get a comprehensive answer that combines both document content and external search results
4. **Toggle Control**: Enable/disable external search via the sidebar toggle

### Search Modes

- **üîç External Search Enabled**: AI will search external sources when document context is insufficient
- **üìö Document Only Mode**: AI will only use information from your uploaded documents

### Example Use Cases

**Questions that trigger external search:**

- "What are the latest developments in [topic not in your document]?"
- "What happened after [date mentioned in document]?"
- "How does this compare to recent industry trends?"
- "What are the current market conditions for [topic]?"

**Questions that use document only:**

- Direct questions about content explicitly mentioned in your uploaded PDF
- Summaries of document sections
- Analysis of data/charts within the document

## üìã Requirements

- Python 3.11+
- Required packages (from pyproject.toml):
  ```
  langchain-anthropic>=0.3.13
  langchain-chroma>=0.2.3
  langchain-community>=0.3.24
  langchain-core>=0.3.59
  langchain-ollama>=0.3.2
  langchain-openai>=0.3.16
  langchain-tavily>=0.2.4
  langchain-text-splitters>=0.3.8
  pymupdf>=1.25.5
  protobuf<=3.20.3
  python-dotenv>=1.1.0
  streamlit>=1.45.1
  ```
- API keys for OpenAI and Anthropic (for cloud models)
- **Tavily API key** (for external search functionality)
- Ollama installation (for local models)

## üîß Setup

1. Clone this repository
2. Install the required packages using uv package manager:
   ```
   uv pip install .
   ```
3. Create a `.env` file with your API keys:
   ```
   OPENAI_API_KEY=your_openai_api_key
   ANTHROPIC_API_KEY=your_anthropic_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```
4. Get a **Tavily API key** from [https://tavily.com/](https://tavily.com/) for external search functionality
5. For local models, install Ollama and pull the DeepSeek models:
   ```
   ollama pull deepseek-r1:1.5b
   ```
6. Create a directory for document storage:
   ```
   mkdir -p document_store/pdfs
   ```

## üöÄ Usage

### Cloud Version (No Setup Required)

Simply visit **[https://docu-chat-ai.streamlit.app/](https://docu-chat-ai.streamlit.app/)** to start using the application immediately!

### Local Installation

#### GPT-4o and Claude Sonnet 4 Version (ChromaDB)

```
streamlit run rag_gpt_claude.py
```

#### GPT-4o and Claude Sonnet 4 Version (Legacy InMemoryVectorStore)

```
streamlit run rag_gpt_claude_old_approach.py
```

#### DeepSeek R1 Version

```
streamlit run rag_deepseek_r1.py
```

### How to Use

1. Upload a PDF document using the file uploader
2. **Choose your search mode**: Toggle external search ON/OFF in the sidebar
3. Wait for the document to be processed, chunked, and indexed
4. Ask questions about the document content using the chat interface
5. **Enhanced Responses**: Get AI-generated responses that combine document content with external search results (when enabled)
6. Use the "Clear Chat History" button in the sidebar to start a new conversation

## üåê Cloud Deployment

### Streamlit Community Cloud

The application is deployed on **Streamlit Community Cloud** and accessible at:
**üîó [https://docu-chat-ai.streamlit.app/](https://docu-chat-ai.streamlit.app/)**

#### Key Features for Cloud Deployment

‚úÖ **InMemoryVectorStore** - No persistence issues on cloud platforms  
‚úÖ **Temporary file handling** - Cloud-compatible PDF processing  
‚úÖ **Session state tracking** - Remembers processed files during your session  
‚úÖ **Cached resources** - Optimized performance  
‚úÖ **Error handling** - Robust cloud deployment with graceful fallbacks  
‚úÖ **Graceful fallbacks** - Works even without external search API keys  

#### Cloud Limitations

‚ö†Ô∏è **File persistence:** Documents reset when app restarts (session-based storage)  
‚ö†Ô∏è **Concurrent users:** ~3-5 users simultaneously on free tier  
‚ö†Ô∏è **Resource limits:** 1GB memory limit on Streamlit Community Cloud  
‚ö†Ô∏è **External search:** Limited functionality without Tavily API key  

#### Deploy Your Own Instance

1. **Fork this repository** on GitHub
2. **Deploy on Streamlit Cloud:**
   - Go to [share.streamlit.io/new](https://share.streamlit.io/new)
   - Select your forked repository
   - **Main file path:** `streamlit_cloud.py`
3. **Set environment variables:**
   ```
   OPENAI_API_KEY=your_openai_api_key_here
   ANTHROPIC_API_KEY=your_anthropic_api_key_here
   TAVILY_API_KEY=your_tavily_api_key_here (optional)
   ```

**Note:** External search will work with limited functionality without the Tavily API key. Get a free API key at [tavily.com](https://tavily.com) for full external search capability.

## üìä Comparison of Available Models

| Model           | Type  | Best For                                       | Required Setup      | Implementation                            | External Search |
| --------------- | ----- | ---------------------------------------------- | ------------------- | ----------------------------------------- | --------------- |
| GPT-4o          | Cloud | High accuracy, complex queries                 | OpenAI API key      | ChatOpenAI with temperature=0             | ‚úÖ              |
| GPT-4.1         | Cloud | Latest OpenAI model with improved reasoning    | OpenAI API key      | ChatOpenAI with temperature=0             | ‚úÖ              |
| Claude Sonnet 4 | Cloud | Latest Claude model with enhanced capabilities | Anthropic API key   | ChatAnthropic with temperature=0          | ‚úÖ              |
| DeepSeek R1     | Local | Privacy, offline use, faster responses         | Ollama installation | OllamaLLM with the deepseek-r1:1.5b model | ‚úÖ              |

## üîç Embedding Models

The application uses different embedding models based on the version:

- **Cloud Version**: OpenAI's `text-embedding-3-large` model for high-quality embeddings
- **Local Version**: Can use either OpenAI embeddings or local Ollama embeddings with DeepSeek models

## üîí Privacy

### Cloud Deployment (Streamlit Community Cloud)
- **Document processing**: Files are temporarily processed on Streamlit servers
- **No persistence**: Documents are deleted when session ends or app restarts
- **API calls**: Document content sent to OpenAI/Anthropic APIs for processing
- **External search**: Search queries sent to Tavily API when enabled

### Local Deployment
- **Complete privacy**: All document processing happens on your machine
- **Local storage**: Documents stored in local ChromaDB (persistent)
- **API calls**: Only model responses sent to external APIs (OpenAI/Anthropic)
- **DeepSeek R1**: Fully local processing with no external API calls

### Model Privacy Levels
- **Cloud models** (GPT-4o, GPT-4.1, Claude Sonnet 4): Document content sent to external APIs
- **Local models** (DeepSeek R1): All processing happens locally with no data leaving your machine
- **External Search**: When enabled, search queries are sent to Tavily API for external information

## üóÑÔ∏è Vector Storage Options

### ChromaDB (Current Approach)

- **Persistent Storage**: Document embeddings are saved to disk
- **Better Scalability**: Can handle larger document collections
- **Improved Performance**: Optimized for efficient retrieval
- **Implementation**: Used in `rag_gpt_claude.py`

### InMemoryVectorStore (Legacy Approach)

- **Lightweight**: Simple in-memory storage with no persistence
- **Fast for Small Datasets**: Efficient for smaller document collections
- **Implementation**: Used in `rag_gpt_claude_old_approach.py`

## ü§ñ External Search Agent Architecture

The application uses an intelligent agent system for external search:

- **Agent Framework**: LangChain ReAct agent pattern
- **Tools**: Tavily Search API integration
- **Orchestration**: Automatic fallback when document context is insufficient
- **Response Enhancement**: Combines document and external contexts intelligently

### Example Workflow

1. Upload a research paper about AI from 2023
2. Ask: "What are the latest AI developments in 2024?"
3. Watch as the AI:
   - Recognizes the document doesn't contain 2024 information
   - Automatically searches external sources via Tavily
   - Provides a comprehensive answer combining both sources

## üéØ Benefits of Enhanced Integration

- **No more "I don't know" responses**: AI can find relevant information beyond your documents
- **Always current information**: Get the most up-to-date data from external sources
- **Seamless integration**: No manual switching between modes
- **Clear source indication**: Know what information comes from documents vs. external sources
- **Intelligent fallback**: External search only triggers when needed

## Project Screenshots

In this demo I used the BPS Palu City data of [population and employment](https://github.com/mcikalmerdeka/NLP-Learning/blob/main/Langchain%20Document%20Assistant%20with%20Deepseek%20R1%20-%20GPT4o%20-%20Claude%203.5%20Sonnet/document_store/pdfs/Statistik%20Penduduk%20dan%20Ketenagakerjaan%20Kota%20Palu%202025.pdf) from the official report of "Kota Palu Dalam Angka 2025" which you can access in this `document_store` folder in this repo. You may use several example files that I stored there or use your own PDFs.

### Document Upload Interface

![Upload Document](./assets/Project%20Screenshot%201.png)

*The document upload interface allows users to select and upload PDF files for analysis. The system processes the document and prepares it for question answering with optional external search capabilities.*

### Initial Question and Answer

![Document Chat](./assets/Project%20Screenshot%202.png)

*After document processing, users can ask specific questions about the content. The AI assistant retrieves relevant information from the document and provides comprehensive answers based on the document context, with the option to enhance responses using external search.*

### Follow-up Question Capabilities

![Follow-up Questions](./assets/Project%20Screenshot%203.png)

*The system supports follow-up questions, maintaining context from previous interactions. This allows for a natural conversation flow while exploring document content in greater depth, with intelligent external search integration when document context is insufficient.*
