{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import the necessary modules for models\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Langchain Ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model 'google/flan-t5-small' doesn't support task 'text2text-generation'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(\n\u001b[0;32m      5\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# smaller, supported model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACEHUB_API_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you still have fun?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     ]\n\u001b[1;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 784\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    788\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    793\u001b[0m         )\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1523\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1522\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1523\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1526\u001b[0m     )\n\u001b[0;32m   1527\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:267\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[0;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:302\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use `post` with another provider than `hf-inference`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`InferenceClient.post` is deprecated and should not be used directly anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    301\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m HFInferenceTask(task \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 302\u001b[0m mapped_model \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_mapped_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m url \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    304\u001b[0m headers \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_headers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\hf_inference.py:35\u001b[0m, in \u001b[0;36mHFInferenceTask._prepare_mapped_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no recommended model for HF Inference. Please specify a model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explicitly. Visit https://huggingface.co/tasks for more info.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[1;32m---> 35\u001b[0m \u001b[43m_check_supported_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_id\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\hf_inference.py:147\u001b[0m, in \u001b[0;36m_check_supported_task\u001b[1;34m(model, task)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support task \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_tag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-text-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_conversational \u001b[38;5;129;01mand\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversational\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Model 'google/flan-t5-small' doesn't support task 'text2text-generation'."
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    # repo_id=\"tiiuae/falcon-7b-instruct\", # larger, but right now unsupported model (from course)\n",
    "    repo_id=\"google/flan-t5-small\",  # smaller, supported model\n",
    "    task=\"text2text-generation\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "question = \"Can you still have fun?\"\n",
    "output = llm.invoke(question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " without alcohol?\n",
      "\n",
      "Yes, there are many ways to have fun without alcohol. Some examples include spending time with friends and family, engaging in hobbies or activities, trying new things, traveling, and exploring new places. Additionally, there are many alcohol-free activities and events such as concerts, sports games, festivals, and art exhibitions that can be enjoyed without the need for alcohol. It is also possible to have a good time and socialize without drinking by finding other non-alcoholic drinks to enjoy, participating in non-drinking games or activities, or simply enjoying good conversation and company.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    # model=\"gpt-4.1-2025-04-14\",\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "question = \"Can you still have fun\"\n",
    "output = llm.invoke(question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that the models used need to support the chat completion for this approach. Which is why when i used the newest 4.1 model, it will return this error message:\n",
    "\n",
    "`NotFoundError: Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prompting Strategies For Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Translate the following into French: I love programming in Python', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Prompt templates\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate the following into {language}: {text}\")\n",
    "prompt.format(language=\"French\", text=\"I love programming in Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1:\n",
      "text='\\nYou are an artificial intelligence assistant, answer the question. What is Langchain?\\n'\n",
      "\n",
      "Version 2:\n",
      "text='Answer the following question:\\nWhat is Langchain?'\n"
     ]
    }
   ],
   "source": [
    "# Prompt templates\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an artificial intelligence assistant, answer the question. {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "print(\"Version 1:\")\n",
    "print(prompt_template.invoke({\"question\" : \"What is Langchain?\"}))\n",
    "\n",
    "# Another approach\n",
    "# Prompt template to format question into a string\n",
    "prompt_template = PromptTemplate.from_template(\"Answer the following question:\\n{question}\")\n",
    "print(\"\\nVersion 2:\")\n",
    "print(prompt_template.invoke({\"question\" : \"What is Langchain?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important thing to notice is that in the .invoke() method, the variable in the prompt template must be specified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-6814c4e7-35758dc85fac35c905356813;143bda33-9356-4927-9987-0c45ab5f8617)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/google/flan-t5-xl.\nMake sure your token has the correct permissions.\nThe model google/flan-t5-xl is too large to be loaded automatically (11GB > 10GB).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://router.huggingface.co/hf-inference/models/google/flan-t5-xl",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m|\u001b[39m llm \n\u001b[0;32m     12\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Langchain?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     ]\n\u001b[1;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 784\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    788\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    793\u001b[0m         )\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1523\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1522\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1523\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1526\u001b[0m     )\n\u001b[0;32m   1527\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:267\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[0;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    303\u001b[0m url \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    304\u001b[0m headers \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_headers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Cikal Merdeka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:473\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    468\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m     )\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[0;32m    476\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-6814c4e7-35758dc85fac35c905356813;143bda33-9356-4927-9987-0c45ab5f8617)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/google/flan-t5-xl.\nMake sure your token has the correct permissions.\nThe model google/flan-t5-xl is too large to be loaded automatically (11GB > 10GB)."
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-xl\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# Chain the prompt template to the model using langchain expression language (LCEL)\n",
    "# Chain: connect calls to LLMs and other components\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "# Run the chain\n",
    "question = \"What is Langchain?\"\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat models\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # System message: Sets the behavior/role of the AI assistant\n",
    "        (\"system\", \"You are soto zen master Roshi\"),\n",
    "        \n",
    "        # Human message: Represents a message from the user in the conversation history\n",
    "        (\"human\", \"What is the essence of Zen\"),\n",
    "        \n",
    "        # AI message: Represents a previous response from the AI in the conversation history\n",
    "        # This acts as an example of how the AI should respond\n",
    "        (\"ai\", \"When you are hungry, eat. When you are thirsty, drink. When you are tired, sleep.\"),\n",
    "        \n",
    "        # Human message with a variable: This is where the actual user question will be inserted\n",
    "        # The {question} placeholder will be replaced with the actual question at runtime\n",
    "        (\"human\", \"Respond to the question: {question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When anger arises, pause and breathe. Observe the sensations in your body and the thoughts in your mind without judgment. Allow the energy of the anger to flow through you. Remember that anger is a natural emotion; it is a response, not a directive. You can acknowledge it, but do not let it control your actions. Find a quiet space if you can, and return to your breath. With each exhale, let go of the intensity. In that stillness, clarity will arise. Respond with wisdom rather than reflex.\n"
     ]
    }
   ],
   "source": [
    "# Integrating ChatPromptTemplate to LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Chain the prompt template to the model\n",
    "llm_chain = prompt_template | llm\n",
    "question = \"What should I do when I'm angry?\"\n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When anger arises, acknowledge it without judgment. Sit quietly and breathe deeply. Allow the energy of anger to be present, but do not let it control you. Observe the sensations in your body, the thoughts in your mind, and the feelings in your heart. \n",
      "\n",
      "Let it come and let it go, like clouds passing across the sky. Instead of reacting, cultivate awareness. Find a space of stillness within. You might then choose to act from compassion, understanding, or simply allow the moment to pass. In the midst of anger, the practice is to return to the breath and find your center.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Another approach is by format the prompt question directly\n",
    "# Notice that in here we don't need to make the chain\n",
    "formatted_prompt = prompt_template.format_messages(question=\"What should I do when I'm angry?\")\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Run the model\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print the output\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Few-Show Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-Shot Prompting: Providing multiple examples to guide model behavior\n",
    "\n",
    "    - Zero-shot: No examples, just instructions (e.g., \"Translate this to French\")\n",
    "    - One-shot: Single example before the actual task\n",
    "    - Few-shot: Multiple examples (like we're doing here)\n",
    " \n",
    "FewShotPromptTemplate is superior to regular PromptTemplate because it:\n",
    "\n",
    "    1. Handles formatting of multiple examples automatically\n",
    "    2. Maintains consistent structure across examples\n",
    "    3. Separates example logic from the main prompt\n",
    "    4. Allows dynamic selection of examples based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Italy?\n",
      "Rome\n"
     ]
    }
   ],
   "source": [
    "# Formatting the examples\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "prompt = example_prompt.invoke({\"question\": \"What is the capital of Italy?\", \"answer\": \"Rome\"})\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Italy?\n",
      "Rome\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Paris\n",
      "\n",
      "Question: What is the capital of Germany?\n",
      "Berlin\n",
      "\n",
      "Question: What is the capital of Spain?\n",
      "Madrid\n",
      "\n",
      "Question: What is the capital of Portugal?\n",
      "Lisbon\n",
      "\n",
      "Question: What is the capital of Greece?\n",
      "Athens\n",
      "\n",
      "Question: What is the capital of Turkey?\n",
      "Ankara\n",
      "\n",
      "Question: What is the capital of the United States?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Building an examples set\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of Italy?\", \"answer\": \"Rome\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n",
    "    {\"question\": \"What is the capital of Spain?\", \"answer\": \"Madrid\"},\n",
    "    {\"question\": \"What is the capital of Portugal?\", \"answer\": \"Lisbon\"},\n",
    "    {\"question\": \"What is the capital of Greece?\", \"answer\": \"Athens\"},\n",
    "    {\"question\": \"What is the capital of Turkey?\", \"answer\": \"Ankara\"},\n",
    "]\n",
    "\n",
    "# Convert Dataframe to list of dicts\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(examples)\n",
    "examples = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Using FewShotPromptTemplate\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,                      # examples: the list of dicts\n",
    "    example_prompt=example_prompt,          # example_prompt: formatted template\n",
    "    suffix=\"Question: {input}\\nAnswer:\",    # suffix: suffix to add to the input\n",
    "    input_variables=[\"input\"]               # input_variables: the input variables\n",
    ")\n",
    "\n",
    "# Invoking the few shot prompt template\n",
    "response = prompt_template.invoke({\"input\": \"What is the capital of the United States?\"})\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# Integration with a chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"What is the capital of the United States?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some probems can only be solved sequentially, for example we can ask a model about where are the places to visit in Paris and then the model will give several recommendations, and then we as the user will tell the model which activities to select and make it plan for us. Notice that this requires more than one user input (sequential), first is to specify the destination and another one to select activities.\n",
    "\n",
    "- The output from one chain becomes the input to another chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Heres a one-day itinerary that covers three top recommendations in Jakarta, giving you a blend of cultural experiences, sightseeing, and local flavors.\n",
      "\n",
      "### One-Day Jakarta Itinerary\n",
      "\n",
      "#### Morning\n",
      "**Start at Monas (National Monument)**\n",
      "- **Time**: 8:00 AM - 9:30 AM\n",
      "- **Description**: Begin your day by visiting Monas, the iconic symbol of Indonesia's independence. Take the elevator to the top for panoramic views of the bustling city. Be sure to stroll around the surrounding park for some fresh air and beautiful gardens.\n",
      "\n",
      "**Head to Istiqlal Mosque**\n",
      "- **Time**: 9:45 AM - 10:30 AM\n",
      "- **Description**: A short distance away, visit the Istiqlal Mosque, Southeast Asias largest mosque. Admire its remarkable architecture and serene atmosphere. If youre interested, you can partake in a guided tour to learn more about the mosque's significance.\n",
      "\n",
      "**Visit Jakarta Cathedral**\n",
      "- **Time**: 10:45 AM - 11:30 AM\n",
      "- **Description**: Right next to Istiqlal, explore the Jakarta Cathedral, a stunning neo-gothic architectural marvel. Take a moment to appreciate its tranquil ambiance and impressive design.\n",
      "\n",
      "#### Lunch\n",
      "**Culinary Tour in Jalan Sabang**\n",
      "- **Time**: 12:00 PM - 1:30 PM\n",
      "- **Description**: Head to Jalan Sabang for lunch and dive into Jakarta's street food scene. Try local specialties such as Nasi Goreng (fried rice), Sate (skewered meat), and perhaps some Gado-Gado (vegetable salad with peanut sauce) from various street vendors.\n",
      "\n",
      "#### Afternoon\n",
      "**Explore Kota Tua (Old Town)**\n",
      "- **Time**: 1:45 PM - 3:30 PM\n",
      "- **Description**: Travel to Kota Tua, where the rich history of Jakarta unfolds. Visit the Fatahillah Museum and wander through the historical streets. Enjoy a coffee at one of the charming cafes and take in the colonial architecture.\n",
      "\n",
      "**Visit Museum Nasional (National Museum)**\n",
      "- **Time**: 3:45 PM - 5:00 PM\n",
      "- **Description**: Conclude your afternoon by immersing yourself in Indonesias history at the Museum Nasional. Explore its extensive collection of artifacts and exhibits which reflect the diverse culture and heritage of the nation.\n",
      "\n",
      "#### Evening\n",
      "**Dinner at Grand Indonesia Mall**\n",
      "- **Time**: 6:00 PM - 8:00 PM\n",
      "- **Description**: Head to Grand Indonesia Mall for dinner. You have a vast array of restaurants to choose from. Try local Indonesian cuisine or opt for international dishes. Enjoy some shopping or just window shopping at the mall afterward.\n",
      "\n",
      "#### Night\n",
      "**Enjoy Jakarta's Nightlife**\n",
      "- **Time**: 8:30 PM - Late\n",
      "- **Description**: Wrap up your day by exploring the vibrant nightlife. Consider visiting bars in Kemang or Senopati known for their live music and lively atmosphere, or simply take a stroll along the streets to experience the local scene.\n",
      "\n",
      "### Tips\n",
      "- **Transportation**: Use ride-hailing apps for ease of movement, especially considering Jakartas traffic.\n",
      "- **Time Management**: Keep an eye on the time as some attractions may have queues or require travel time.\n",
      "- **Dress Code**: Dress modestly when visiting places of worship.\n",
      "\n",
      "Enjoy your day exploring the vibrant city of Jakarta! Let me know if you have any further questions or need more details!\n"
     ]
    }
   ],
   "source": [
    "# Create 2 prompt template\n",
    "destination_prompt = PromptTemplate(\n",
    "    input_variables=[\"destination\"],\n",
    "    template=\"\"\"\n",
    "    I am planning a trip to {destination}. Can you suggest some activities to do there?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "activity_prompt = PromptTemplate(\n",
    "    input_variables=[\"activities\"],\n",
    "    template=\"\"\"\n",
    "    I only have one day, so can you create an itinerary from your top three recommendations: {activities}?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Import the StrOutputParser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Create the sequential chain using the StrOutputParser\n",
    "sequential_chain = ({\"activities\": destination_prompt | llm | StrOutputParser()}\n",
    "                    | activity_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = sequential_chain.invoke({\"destination\": \"Jakarta\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, `StrOutputParser()` is used to **convert the output of the language model (LLM) into a plain Python string**. Here's a breakdown of what it's doing:\n",
    "\n",
    "- Purpose of `StrOutputParser()`:\n",
    "\n",
    "    - It **parses the raw output** from the LLM and **returns it as a standard string**, which is necessary because LLM output might be in a more complex format (e.g. a JSON-like structure, or a response object).\n",
    "    - It ensures that the next stage in the chain receives a **simple, clean string** as input.\n",
    "\n",
    "- Example of its role in the chain:\n",
    "\n",
    "```python\n",
    "{\"activities\": destination_prompt | llm | StrOutputParser()}\n",
    "```\n",
    "    - `destination_prompt` generates a prompt.\n",
    "    - `llm` (e.g., a model like GPT-4) responds to the prompt.\n",
    "    - `StrOutputParser()` takes the response and extracts the actual text output (as a string).\n",
    "    - The resulting string is assigned to the key `\"activities\"`.\n",
    "\n",
    "This parsed `\"activities\"` string is then fed into the second half of the chain:\n",
    "\n",
    "```python\n",
    "| activity_prompt | llm | StrOutputParser()\n",
    "```\n",
    "\n",
    " which generates the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Introduction to Langchain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agents: use LLMs to take actions\n",
    "- Tools: functions called by the agent\n",
    "\n",
    "In this tutorial we will be using ReAct(reason + act) agents.\n",
    "\n",
    "Example would be like:\n",
    "\n",
    "user: What is the weather like in Kingston, Jamaica?\n",
    "agent: \n",
    "\n",
    "    - Thought: I should call Weather() to find the weather in Kingston, Jamaica\n",
    "    - Act: Weather(\"Kingston, Jamaica\")\n",
    "    - Observe: The weather is mostly sunny with temperatures of 82F\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement agents, we will be using LangGraph, which is a branch of Langchain ecosystem **centered around designing agent system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the square root of 101?', additional_kwargs={}, response_metadata={}, id='cdb811fd-6a16-4e65-855a-b724adb8b26a'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wRsMFdhXHSI7RBc2ZCh3WaQF', 'function': {'arguments': '{\"__arg1\":\"sqrt(101)\"}', 'name': 'Calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 63, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f64fcf55-28e5-4f82-b13d-5ee53875276d-0', tool_calls=[{'name': 'Calculator', 'args': {'__arg1': 'sqrt(101)'}, 'id': 'call_wRsMFdhXHSI7RBc2ZCh3WaQF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 63, 'output_tokens': 20, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content='Answer: 10.04987562112089', name='Calculator', id='ad26d772-f5e5-4e81-9341-67c39e275907', tool_call_id='call_wRsMFdhXHSI7RBc2ZCh3WaQF'),\n",
      "              AIMessage(content='The square root of 101 is approximately 10.05.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 99, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'finish_reason': 'stop', 'logprobs': None}, id='run-68e8e3a0-b561-4e0e-86eb-32f02d0aa690-0', usage_metadata={'input_tokens': 99, 'output_tokens': 15, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# ReAct agent\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "import pprint\n",
    "\n",
    "# First rebuild the model\n",
    "LLMMathChain.model_rebuild()\n",
    "\n",
    "# Then create tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"What is the square root of 101?\")]})\n",
    "pprint.pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you can see how the agent calls the tools. Another example below is by utilizing the memory so that the ReAct agent can remember previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response:\n",
      "The square root of 101 is approximately 10.05.\n",
      "\n",
      "\n",
      "Second response:\n",
      "The number 10.05 multiplied by 15 is 150.75.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# First rebuild the model\n",
    "LLMMathChain.model_rebuild()\n",
    "\n",
    "# Create the tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Create a conversation memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Create the agent with memory\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Initialize conversation with empty list\n",
    "conversation_history = []\n",
    "\n",
    "# Function to interact with the agent while maintaining memory\n",
    "def interact_with_agent(user_input):\n",
    "    global conversation_history\n",
    "    \n",
    "    # Add the new user message to history\n",
    "    conversation_history.append((\"human\", user_input))\n",
    "    \n",
    "    # Create the input with all history\n",
    "    agent_input = {\"messages\": conversation_history}\n",
    "    \n",
    "    # Get the agent's response\n",
    "    response = agent.invoke(agent_input)\n",
    "    \n",
    "    # Extract the latest assistant message and add it to history\n",
    "    latest_assistant_message = response[\"messages\"][-1]\n",
    "    conversation_history.append(latest_assistant_message)\n",
    "    \n",
    "    # Return the content of the message, not trying to index it\n",
    "    return latest_assistant_message.content\n",
    "\n",
    "# Example usage\n",
    "response1 = interact_with_agent(\"What is the square root of 101?\")\n",
    "print(\"First response:\")\n",
    "print(response1)\n",
    "print(\"\\n\")\n",
    "\n",
    "# The agent now remembers the previous interaction\n",
    "response2 = interact_with_agent(\"And what is that number multiplied by 15?\")\n",
    "print(\"Second response:\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent remember the answer from previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Custom Tools For Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools must be formatted in a specific way to be compatible with agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "tools = load_tools([\"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculator\n",
      "Useful for when you need to answer questions about math.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# First they must have a name accessible via the name attribute\n",
    "print(tools[0].name)\n",
    "\n",
    "# A description which is used by the LLM/agent as contenxt to determine when to call it\n",
    "print(tools[0].description)\n",
    "\n",
    "# Return direct parameter idicate whether the agent should stop after invoking this tool\n",
    "print(tools[0].return_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom function\n",
    "def financial_report(company_name: str, revenue:int, expenses:int) -> str:\n",
    "    \"\"\"\n",
    "    Generate a financial report for a company that calculates new income.\n",
    "    \"\"\"\n",
    "\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    # Using string concatenation\n",
    "    report = f\"Financial Report for {company_name}: \\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net: ${net_income}\\n\"\n",
    "\n",
    "#     # Using triple-quoted string\n",
    "#     report = f\"\"\"Financial Report for {company_name}:\n",
    "# Revenue: ${revenue}\n",
    "# Expenses: ${expenses}\n",
    "# Net: ${net_income}\n",
    "# \"\"\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Report for Lemonade Stand: \n",
      "Revenue: $100\n",
      "Expenses: $50\n",
      "Net: $50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(financial_report(company_name=\"Lemonade Stand\", revenue=100, expenses=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert this function into a tool that our agent can call, which will be done using the tool decorator. The @tool modifies the function so that it is in the correct format to be used by tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def financial_report(company_name: str, revenue:int, expenses:int) -> str:\n",
    "    \"\"\"\n",
    "    Generate a financial report for a company that calculates new income.\n",
    "    \"\"\"\n",
    "\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    # Using string concatenation\n",
    "    report = f\"Financial Report for {company_name}: \\n\"\n",
    "    report += f\"Revenue: ${revenue}\\n\"\n",
    "    report += f\"Expenses: ${expenses}\\n\"\n",
    "    report += f\"Net: ${net_income}\\n\"\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financial_report\n",
      "Generate a financial report for a company that calculates new income.\n",
      "False\n",
      "{'company_name': {'title': 'Company Name', 'type': 'string'}, 'revenue': {'title': 'Revenue', 'type': 'integer'}, 'expenses': {'title': 'Expenses', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "# Examining our new tool\n",
    "print(financial_report.name)\n",
    "print(financial_report.description)\n",
    "print(financial_report.return_direct)\n",
    "\n",
    "# or we can access the .args to print the arguments and expected data types \n",
    "print(financial_report.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='TechStack generated made $10 million with $8 million of costs. Generate a financial report.', additional_kwargs={}, response_metadata={}, id='af6c8209-bced-4c0f-b8c5-9ed636ccf550'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TcmMAbPff4rZjN4qB08Wowbk', 'function': {'arguments': '{\"company_name\":\"TechStack\",\"revenue\":10000000,\"expenses\":8000000}', 'name': 'financial_report'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 77, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ddb4ed38-6375-41dc-b2b4-15d6c612f662-0', tool_calls=[{'name': 'financial_report', 'args': {'company_name': 'TechStack', 'revenue': 10000000, 'expenses': 8000000}, 'id': 'call_TcmMAbPff4rZjN4qB08Wowbk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 77, 'output_tokens': 31, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content='Financial Report for TechStack: \\nRevenue: $10000000\\nExpenses: $8000000\\nNet: $2000000\\n', name='financial_report', id='a5ddbd68-46f5-4368-a760-3c1d70d1a686', tool_call_id='call_TcmMAbPff4rZjN4qB08Wowbk'),\n",
      "              AIMessage(content='**Financial Report for TechStack:**\\n\\n- **Revenue:** $10,000,000\\n- **Expenses:** $8,000,000\\n- **Net Income:** $2,000,000', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 143, 'total_tokens': 186, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_129a36352a', 'finish_reason': 'stop', 'logprobs': None}, id='run-5dc9fd75-d56e-434c-ae84-6b8e73b63e96-0', usage_metadata={'input_tokens': 143, 'output_tokens': 43, 'total_tokens': 186, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# Integrating the custom tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import pprint\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create the agent with custom tool\n",
    "agent = create_react_agent(llm, [financial_report])\n",
    "\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\")]})\n",
    "pprint.pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Financial Report for TechStack:**\n",
      "\n",
      "- **Revenue:** $10,000,000\n",
      "- **Expenses:** $8,000,000\n",
      "- **Net Income:** $2,000,000\n"
     ]
    }
   ],
   "source": [
    "# Get only the ai response\n",
    "final_response = messages[\"messages\"][-1].content\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Integrating Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note to self: this is a Retrival Augmented Generation(RAG) implementation. For more detailed please later refer to the RAG with Langchain course in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-trained language models don't have access to external data sources. Their understanding comes purely from training data. This means that if we require the model to have knowledge that goes beyond its training data, which could be company data or more recent world events, you need a way to integrating that data.\n",
    "\n",
    "- In RAG, user query is embedded and used to retrieve the relevant document fromt he database. Then these documents are added to the models prompt, so that the modelhave extra context to inform its response.\n",
    "\n",
    "- There are 3 RAG development steps in Langchain\n",
    "\n",
    "    1. Document Loader (load into langchain)\n",
    "    2. Splitting (into chunks -> unit of information that we can index and process individually)\n",
    "    3. Storage + Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain document loaders:\n",
    "\n",
    "    - Classes designed to load and configure documents for system integration\n",
    "    - Dcoument loader for common file types: pdf and csv\n",
    "    - Additional loaders provided by 3rd parties for managing unique document formats: amazon S3 files, ipynb, wav, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Understanding the Rise of AI-Powered Tools in Modern Workflows \n",
      "In recent years, artificial intelligence (AI) has transformed how organizations handle data, \n",
      "automate processes, and deliver insights. From natural language processing to computer \n",
      "vision, AI models are now integrated into everyday business tools, enhancing productivity and \n",
      "decision-making. Companies are rapidly adopting frameworks like LangChain to build \n",
      "applications that interact with documents, answer questions, and even summarize content in \n",
      "real time. \n",
      "One popular use case is Retrieval -Augmented Generation (RAG), which improves the \n",
      "accuracy and relevance of AI-generated responses by grounding them in real-world data. By \n",
      "combining a language model with a vector store and document loader, RAG systems can \n",
      "retrieve context-specific information from a knowledge base, such as PDFs, to answer user \n",
      "queries more effectively. This approach is especially valuable in domains like legal, healthcare, \n",
      "and finance, where precision and up-to-date information are critical. \n",
      " ' metadata={'source': 'E:\\\\NLP Learning\\\\NLP-Learning\\\\Datacamp Courses\\\\Developing LLM Applications with LangChain\\\\Document Example.pdf', 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# PDF document loader (require installation of the pypdf package)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# loader = PyPDFLoader(\"path/to/your'file.pdf\")\n",
    "loader = PyPDFLoader(r\"E:\\NLP Learning\\NLP-Learning\\Datacamp Courses\\Developing LLM Applications with LangChain\\Document Example.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Name: Michelle Rose\n",
      "Email: carriehall@yahoo.com\n",
      "Company: Wood, Foley and Terry\n",
      "Job Title: Engineer, land\n",
      "Date Joined: 2023-10-29\n",
      "Country: Niger\n",
      "Salary (USD): 129547.35' metadata={'source': 'E:\\\\NLP Learning\\\\NLP-Learning\\\\Datacamp Courses\\\\Developing LLM Applications with LangChain\\\\sample_employee_data.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "# CSV document loader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(r\"E:\\NLP Learning\\NLP-Learning\\Datacamp Courses\\Developing LLM Applications with LangChain\\sample_employee_data.csv\")\n",
    "data = loader.load()\n",
    "\n",
    "# To get just the page_content for a single document\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm actually kinda curious about this document loader of csv, can it directly extract insights from here or not (like you actually need other components first like splitting it into chunks first), we will find out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Date Joined</th>\n",
       "      <th>Country</th>\n",
       "      <th>Salary (USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michelle Rose</td>\n",
       "      <td>carriehall@yahoo.com</td>\n",
       "      <td>Wood, Foley and Terry</td>\n",
       "      <td>Engineer, land</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>Niger</td>\n",
       "      <td>129547.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paul Anderson</td>\n",
       "      <td>alicia56@gmail.com</td>\n",
       "      <td>Evans Ltd</td>\n",
       "      <td>Field seismologist</td>\n",
       "      <td>2024-03-26</td>\n",
       "      <td>Swaziland</td>\n",
       "      <td>114199.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terri Chang</td>\n",
       "      <td>lambertmax@hotmail.com</td>\n",
       "      <td>Watson-Mccoy</td>\n",
       "      <td>Health and safety adviser</td>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>Guernsey</td>\n",
       "      <td>116572.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Larry Williams</td>\n",
       "      <td>crossbrandon@gmail.com</td>\n",
       "      <td>Taylor-Taylor</td>\n",
       "      <td>Solicitor, Scotland</td>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>Martinique</td>\n",
       "      <td>46575.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gabriel Beck</td>\n",
       "      <td>meganball@yahoo.com</td>\n",
       "      <td>Osborn LLC</td>\n",
       "      <td>Designer, television/film set</td>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>100704.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name                   Email                Company  \\\n",
       "0   Michelle Rose    carriehall@yahoo.com  Wood, Foley and Terry   \n",
       "1   Paul Anderson      alicia56@gmail.com              Evans Ltd   \n",
       "2     Terri Chang  lambertmax@hotmail.com           Watson-Mccoy   \n",
       "3  Larry Williams  crossbrandon@gmail.com          Taylor-Taylor   \n",
       "4    Gabriel Beck     meganball@yahoo.com             Osborn LLC   \n",
       "\n",
       "                       Job Title Date Joined     Country  Salary (USD)  \n",
       "0                 Engineer, land  2023-10-29       Niger     129547.35  \n",
       "1             Field seismologist  2024-03-26   Swaziland     114199.87  \n",
       "2      Health and safety adviser  2021-01-29    Guernsey     116572.83  \n",
       "3            Solicitor, Scotland  2022-06-18  Martinique      46575.15  \n",
       "4  Designer, television/film set  2020-07-23     Lebanon     100704.51  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sample_employee_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try 2 simple aggregations and 1 data lookup. As for the agregation we will find the sum of salary and then the count of Albania, while for the lookup we will find the country of empoyee name Gabriel Beck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of employee salary: 7113389.759999999\n",
      "Number of employee from Albania: 2\n",
      "Country of employee Gabriel Beck: Lebanon\n"
     ]
    }
   ],
   "source": [
    "# Check the values that we want the LLMs to find\n",
    "salary_sum = df['Salary (USD)'].sum()\n",
    "albania_count = len(df['Country'].loc[df['Country'] == \"Albania\"])\n",
    "lookup_employee_country = df.loc[df['Name'] == \"Gabriel Beck\", \"Country\"].values[0]\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(f\"Sum of employee salary: {salary_sum}\")\n",
    "print(f\"Number of employee from Albania: {albania_count}\")\n",
    "print(f\"Country of employee Gabriel Beck: {lookup_employee_country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I created the data that I want to check above, I just realized that the retrival should actually be done in the section 9. So in here I will just try to pass it directly into available LLM in cursor to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Michelle Rose\n",
      "Email: carriehall@yahoo.com\n",
      "Company: Wood, Foley and Terry\n",
      "Job Title: Engineer, land\n",
      "Date Joined: 2023-10-29\n",
      "Country: Niger\n",
      "Salary (USD): 129547.35\n",
      "\n",
      "Name: Paul Anderson\n",
      "Email: alicia56@gmail.com\n",
      "Company: Evans Ltd\n",
      "Job Title: Field seismologist\n",
      "Date Joined: 2024-03-26\n",
      "Country: Swaziland\n",
      "Salary (USD): 114199.87\n",
      "\n",
      "Name: Terri Chang\n",
      "Email: lambertmax@hotmail.com\n",
      "Company: Watson-Mccoy\n",
      "Job Title: Health and safety adviser\n",
      "Date Joined: 2021-01-29\n",
      "Country: Guernsey\n",
      "Salary (USD): 116572.83\n",
      "\n",
      "Name: Larry Williams\n",
      "Email: crossbrandon@gmail.com\n",
      "Company: Taylor-Taylor\n",
      "Job Title: Solicitor, Scotland\n",
      "Date Joined: 2022-06-18\n",
      "Country: Martinique\n",
      "Salary (USD): 46575.15\n",
      "\n",
      "Name: Gabriel Beck\n",
      "Email: meganball@yahoo.com\n",
      "Company: Osborn LLC\n",
      "Job Title: Designer, television/film set\n",
      "Date Joined: 2020-07-23\n",
      "Country: Lebanon\n",
      "Salary (USD): 100704.51\n",
      "\n",
      "Name: Emily Ford\n",
      "Email: csanchez@gmail.com\n",
      "Company: Davis and Sons\n",
      "Job Title: Lighting technician, broadcasting/film/video\n",
      "Date Joined: 2021-09-19\n",
      "Country: Djibouti\n",
      "Salary (USD): 73312.73\n",
      "\n",
      "Name: Mr. Matthew Gonzalez\n",
      "Email: kmontgomery@pham-mcguire.org\n",
      "Company: Mcdonald-Ellis\n",
      "Job Title: Quality manager\n",
      "Date Joined: 2020-11-10\n",
      "Country: Hungary\n",
      "Salary (USD): 129488.08\n",
      "\n",
      "Name: James Matthews\n",
      "Email: lindsey36@hotmail.com\n",
      "Company: Hernandez Group\n",
      "Job Title: Structural engineer\n",
      "Date Joined: 2022-12-06\n",
      "Country: Japan\n",
      "Salary (USD): 66374.74\n",
      "\n",
      "Name: Samuel Jones\n",
      "Email: molly66@mosley-taylor.com\n",
      "Company: Smith, Nicholson and Morgan\n",
      "Job Title: Programmer, applications\n",
      "Date Joined: 2023-02-07\n",
      "Country: Chile\n",
      "Salary (USD): 108585.91\n",
      "\n",
      "Name: Robin Long\n",
      "Email: brianamontoya@houston-ray.com\n",
      "Company: Dunlap, Lewis and Brown\n",
      "Job Title: Scientist, forensic\n",
      "Date Joined: 2024-01-16\n",
      "Country: Tajikistan\n",
      "Salary (USD): 43093.32\n",
      "\n",
      "Name: Jaclyn Allen\n",
      "Email: prosales@hotmail.com\n",
      "Company: Blackburn LLC\n",
      "Job Title: Jewellery designer\n",
      "Date Joined: 2021-06-12\n",
      "Country: Uganda\n",
      "Salary (USD): 121494.2\n",
      "\n",
      "Name: Gina Montgomery\n",
      "Email: bobby93@larson-lester.com\n",
      "Company: Boyd-Thompson\n",
      "Job Title: Drilling engineer\n",
      "Date Joined: 2020-12-03\n",
      "Country: South Africa\n",
      "Salary (USD): 107222.7\n",
      "\n",
      "Name: Christine Lewis\n",
      "Email: joymiller@hotmail.com\n",
      "Company: Williams, Dixon and Smith\n",
      "Job Title: Associate Professor\n",
      "Date Joined: 2021-11-23\n",
      "Country: Thailand\n",
      "Salary (USD): 61067.55\n",
      "\n",
      "Name: Matthew Gray\n",
      "Email: timothyweaver@bernard.com\n",
      "Company: Odonnell, Luna and Reynolds\n",
      "Job Title: Magazine journalist\n",
      "Date Joined: 2020-10-01\n",
      "Country: Antigua and Barbuda\n",
      "Salary (USD): 148280.94\n",
      "\n",
      "Name: Dawn Ellison\n",
      "Email: morselarry@moore.net\n",
      "Company: Smith and Sons\n",
      "Job Title: Scientist, water quality\n",
      "Date Joined: 2023-11-17\n",
      "Country: French Southern Territories\n",
      "Salary (USD): 124044.21\n",
      "\n",
      "Name: William Butler\n",
      "Email: smithlisa@peterson.net\n",
      "Company: Marshall, Harris and Brown\n",
      "Job Title: Community education officer\n",
      "Date Joined: 2020-10-05\n",
      "Country: Cocos (Keeling) Islands\n",
      "Salary (USD): 59602.0\n",
      "\n",
      "Name: Jennifer Moreno\n",
      "Email: shawnjenkins@gmail.com\n",
      "Company: Parker, Walker and Cardenas\n",
      "Job Title: Adult nurse\n",
      "Date Joined: 2024-03-02\n",
      "Country: Sri Lanka\n",
      "Salary (USD): 149087.1\n",
      "\n",
      "Name: Michael Johnson\n",
      "Email: fcaldwell@clark.net\n",
      "Company: Hill Inc\n",
      "Job Title: Photographer\n",
      "Date Joined: 2024-02-24\n",
      "Country: Hong Kong\n",
      "Salary (USD): 84008.02\n",
      "\n",
      "Name: Susan Newman\n",
      "Email: gonzalezmeredith@martinez-mccoy.com\n",
      "Company: Hayes-Vaughn\n",
      "Job Title: Financial trader\n",
      "Date Joined: 2023-06-16\n",
      "Country: Indonesia\n",
      "Salary (USD): 117531.07\n",
      "\n",
      "Name: Theresa Morris\n",
      "Email: qholt@howard.com\n",
      "Company: Young, Bradley and Bauer\n",
      "Job Title: Web designer\n",
      "Date Joined: 2022-02-16\n",
      "Country: Philippines\n",
      "Salary (USD): 58948.34\n",
      "\n",
      "Name: Andrew Nielsen\n",
      "Email: rhonda73@gmail.com\n",
      "Company: Rodgers and Sons\n",
      "Job Title: Electrical engineer\n",
      "Date Joined: 2022-07-27\n",
      "Country: Burkina Faso\n",
      "Salary (USD): 114177.97\n",
      "\n",
      "Name: Robert Thompson\n",
      "Email: agarcia@hotmail.com\n",
      "Company: Harrison-Hammond\n",
      "Job Title: Cytogeneticist\n",
      "Date Joined: 2024-05-30\n",
      "Country: Algeria\n",
      "Salary (USD): 111839.3\n",
      "\n",
      "Name: David Ortega\n",
      "Email: clinton05@casey-dean.com\n",
      "Company: Thomas-Daniels\n",
      "Job Title: Further education lecturer\n",
      "Date Joined: 2023-07-05\n",
      "Country: Portugal\n",
      "Salary (USD): 144134.08\n",
      "\n",
      "Name: Eric Gonzales\n",
      "Email: perezanita@dennis.com\n",
      "Company: Chavez, Mitchell and Davis\n",
      "Job Title: Secondary school teacher\n",
      "Date Joined: 2023-02-28\n",
      "Country: Antigua and Barbuda\n",
      "Salary (USD): 146306.02\n",
      "\n",
      "Name: Miguel Cordova\n",
      "Email: regina59@cline.info\n",
      "Company: Smith and Sons\n",
      "Job Title: Risk analyst\n",
      "Date Joined: 2020-12-13\n",
      "Country: Guinea-Bissau\n",
      "Salary (USD): 69844.76\n",
      "\n",
      "Name: Joan Walton\n",
      "Email: stacy10@wood.info\n",
      "Company: Becker Group\n",
      "Job Title: Medical sales representative\n",
      "Date Joined: 2021-10-11\n",
      "Country: United States Virgin Islands\n",
      "Salary (USD): 149617.03\n",
      "\n",
      "Name: Jeremy Brown\n",
      "Email: robertflores@gmail.com\n",
      "Company: Hutchinson Inc\n",
      "Job Title: Education officer, museum\n",
      "Date Joined: 2023-05-21\n",
      "Country: Albania\n",
      "Salary (USD): 143411.7\n",
      "\n",
      "Name: Penny Harper\n",
      "Email: bcarter@yahoo.com\n",
      "Company: Pena Inc\n",
      "Job Title: Regulatory affairs officer\n",
      "Date Joined: 2021-08-18\n",
      "Country: Denmark\n",
      "Salary (USD): 47973.25\n",
      "\n",
      "Name: Steven Carson\n",
      "Email: maciasrobert@miller.com\n",
      "Company: Mcdaniel Ltd\n",
      "Job Title: Textile designer\n",
      "Date Joined: 2023-12-11\n",
      "Country: Bhutan\n",
      "Salary (USD): 67659.78\n",
      "\n",
      "Name: Reginald Evans\n",
      "Email: theresachen@mckay.org\n",
      "Company: Rowland and Sons\n",
      "Job Title: Counsellor\n",
      "Date Joined: 2021-08-26\n",
      "Country: Bosnia and Herzegovina\n",
      "Salary (USD): 49934.49\n",
      "\n",
      "Name: Ashley Kim\n",
      "Email: lee96@yahoo.com\n",
      "Company: Johnson Ltd\n",
      "Job Title: Customer service manager\n",
      "Date Joined: 2022-07-12\n",
      "Country: United States of America\n",
      "Salary (USD): 44290.92\n",
      "\n",
      "Name: Jeremiah Walker\n",
      "Email: sbryant@quinn.com\n",
      "Company: Long Group\n",
      "Job Title: Community development worker\n",
      "Date Joined: 2023-02-14\n",
      "Country: Azerbaijan\n",
      "Salary (USD): 133127.99\n",
      "\n",
      "Name: Thomas Herman\n",
      "Email: prichardson@douglas.com\n",
      "Company: Simmons-Kim\n",
      "Job Title: Hotel manager\n",
      "Date Joined: 2022-08-04\n",
      "Country: French Guiana\n",
      "Salary (USD): 83562.7\n",
      "\n",
      "Name: Jesus Smith\n",
      "Email: robert55@gmail.com\n",
      "Company: King-Sanders\n",
      "Job Title: Community arts worker\n",
      "Date Joined: 2023-10-05\n",
      "Country: Saint Pierre and Miquelon\n",
      "Salary (USD): 128552.02\n",
      "\n",
      "Name: Angela Brown\n",
      "Email: lynchlaura@cobb-jarvis.net\n",
      "Company: Howard PLC\n",
      "Job Title: Senior tax professional/tax inspector\n",
      "Date Joined: 2022-06-13\n",
      "Country: Armenia\n",
      "Salary (USD): 69039.37\n",
      "\n",
      "Name: Travis Gonzalez\n",
      "Email: brittany60@hotmail.com\n",
      "Company: Benitez-Mullen\n",
      "Job Title: Medical laboratory scientific officer\n",
      "Date Joined: 2020-05-22\n",
      "Country: Dominica\n",
      "Salary (USD): 63443.6\n",
      "\n",
      "Name: Linda Knight\n",
      "Email: ryanwallace@yahoo.com\n",
      "Company: Bell-Chavez\n",
      "Job Title: Psychotherapist\n",
      "Date Joined: 2021-08-30\n",
      "Country: Romania\n",
      "Salary (USD): 108401.52\n",
      "\n",
      "Name: Mary Vasquez\n",
      "Email: suzanne58@hotmail.com\n",
      "Company: Mullen and Sons\n",
      "Job Title: Immigration officer\n",
      "Date Joined: 2021-03-24\n",
      "Country: Burundi\n",
      "Salary (USD): 59941.86\n",
      "\n",
      "Name: Barbara Roman\n",
      "Email: jasmineclark@bauer-sullivan.org\n",
      "Company: Martinez-Nelson\n",
      "Job Title: Charity officer\n",
      "Date Joined: 2022-04-28\n",
      "Country: Heard Island and McDonald Islands\n",
      "Salary (USD): 73781.48\n",
      "\n",
      "Name: Jenna Weiss\n",
      "Email: zmoore@pierce-wood.info\n",
      "Company: Williams-Wong\n",
      "Job Title: Metallurgist\n",
      "Date Joined: 2024-12-23\n",
      "Country: United States of America\n",
      "Salary (USD): 62224.7\n",
      "\n",
      "Name: Allison Hanson\n",
      "Email: jamie50@yahoo.com\n",
      "Company: Williams Group\n",
      "Job Title: Facilities manager\n",
      "Date Joined: 2021-11-08\n",
      "Country: New Caledonia\n",
      "Salary (USD): 128661.61\n",
      "\n",
      "Name: Justin Ruiz\n",
      "Email: dnovak@yahoo.com\n",
      "Company: Mcgee Inc\n",
      "Job Title: Regulatory affairs officer\n",
      "Date Joined: 2023-09-15\n",
      "Country: Turkmenistan\n",
      "Salary (USD): 62508.89\n",
      "\n",
      "Name: Erica Graham\n",
      "Email: loganfuentes@gmail.com\n",
      "Company: Fields, Williams and Boyd\n",
      "Job Title: Astronomer\n",
      "Date Joined: 2024-08-05\n",
      "Country: Saint Vincent and the Grenadines\n",
      "Salary (USD): 141384.29\n",
      "\n",
      "Name: William Harper\n",
      "Email: pamela14@hotmail.com\n",
      "Company: Gonzales-Allen\n",
      "Job Title: Commercial/residential surveyor\n",
      "Date Joined: 2020-10-03\n",
      "Country: Uzbekistan\n",
      "Salary (USD): 118350.72\n",
      "\n",
      "Name: Kim Becker\n",
      "Email: wburns@gmail.com\n",
      "Company: Cherry, Campbell and Lopez\n",
      "Job Title: Chartered public finance accountant\n",
      "Date Joined: 2022-12-31\n",
      "Country: Jamaica\n",
      "Salary (USD): 53375.87\n",
      "\n",
      "Name: Mary Sanchez\n",
      "Email: lcisneros@grant.com\n",
      "Company: Warner Ltd\n",
      "Job Title: Electrical engineer\n",
      "Date Joined: 2021-05-05\n",
      "Country: Nepal\n",
      "Salary (USD): 44885.68\n",
      "\n",
      "Name: Mary Lopez\n",
      "Email: otaylor@green-harris.com\n",
      "Company: Watkins-Matthews\n",
      "Job Title: Best boy\n",
      "Date Joined: 2021-07-06\n",
      "Country: Czech Republic\n",
      "Salary (USD): 136844.51\n",
      "\n",
      "Name: David Young\n",
      "Email: lgarcia@yahoo.com\n",
      "Company: Knapp-Guzman\n",
      "Job Title: Designer, blown glass/stained glass\n",
      "Date Joined: 2023-05-10\n",
      "Country: Fiji\n",
      "Salary (USD): 90361.43\n",
      "\n",
      "Name: Nathan Stewart\n",
      "Email: martinhancock@yahoo.com\n",
      "Company: Stewart Ltd\n",
      "Job Title: Advice worker\n",
      "Date Joined: 2024-03-16\n",
      "Country: Uzbekistan\n",
      "Salary (USD): 131536.78\n",
      "\n",
      "Name: Dr. Lori Wright\n",
      "Email: michaelwall@gmail.com\n",
      "Company: Preston, Holden and Anderson\n",
      "Job Title: Psychologist, forensic\n",
      "Date Joined: 2022-09-21\n",
      "Country: Algeria\n",
      "Salary (USD): 97756.05\n",
      "\n",
      "Name: Patrick Miller\n",
      "Email: jamesgordon@walker-mason.com\n",
      "Company: Bean, Martin and Hahn\n",
      "Job Title: Contracting civil engineer\n",
      "Date Joined: 2020-10-19\n",
      "Country: Egypt\n",
      "Salary (USD): 61883.63\n",
      "\n",
      "Name: Joshua Frazier\n",
      "Email: brenda58@gmail.com\n",
      "Company: Perez, Wilson and Sandoval\n",
      "Job Title: Manufacturing engineer\n",
      "Date Joined: 2022-07-11\n",
      "Country: New Caledonia\n",
      "Salary (USD): 78688.13\n",
      "\n",
      "Name: Charles Guzman\n",
      "Email: jeffery72@yahoo.com\n",
      "Company: Nunez, Saunders and Smith\n",
      "Job Title: Occupational therapist\n",
      "Date Joined: 2022-09-11\n",
      "Country: France\n",
      "Salary (USD): 117464.68\n",
      "\n",
      "Name: Melissa Cole\n",
      "Email: alexandercourtney@french.com\n",
      "Company: Beck, Friedman and Copeland\n",
      "Job Title: Retail banker\n",
      "Date Joined: 2021-02-11\n",
      "Country: Turkey\n",
      "Salary (USD): 115737.12\n",
      "\n",
      "Name: Peter Brown\n",
      "Email: antonioking@gmail.com\n",
      "Company: Powell PLC\n",
      "Job Title: Surveyor, minerals\n",
      "Date Joined: 2022-08-24\n",
      "Country: Jordan\n",
      "Salary (USD): 52640.68\n",
      "\n",
      "Name: Allison White\n",
      "Email: gardnercheryl@yahoo.com\n",
      "Company: Johnson, Hill and Smith\n",
      "Job Title: Insurance account manager\n",
      "Date Joined: 2023-06-18\n",
      "Country: French Guiana\n",
      "Salary (USD): 94153.86\n",
      "\n",
      "Name: Margaret Dunn\n",
      "Email: kathryn34@gmail.com\n",
      "Company: Washington-Singh\n",
      "Job Title: Theatre manager\n",
      "Date Joined: 2020-06-30\n",
      "Country: Netherlands Antilles\n",
      "Salary (USD): 93735.22\n",
      "\n",
      "Name: William Wright\n",
      "Email: qvelasquez@gmail.com\n",
      "Company: Spence-Nelson\n",
      "Job Title: Agricultural consultant\n",
      "Date Joined: 2024-10-03\n",
      "Country: Suriname\n",
      "Salary (USD): 71431.28\n",
      "\n",
      "Name: Brent Simpson\n",
      "Email: thill@hotmail.com\n",
      "Company: Swanson-Davis\n",
      "Job Title: English as a second language teacher\n",
      "Date Joined: 2024-06-18\n",
      "Country: Sudan\n",
      "Salary (USD): 118181.1\n",
      "\n",
      "Name: Timothy Kennedy MD\n",
      "Email: htyler@yahoo.com\n",
      "Company: Andrade-Hoover\n",
      "Job Title: Public house manager\n",
      "Date Joined: 2020-10-28\n",
      "Country: Isle of Man\n",
      "Salary (USD): 111396.97\n",
      "\n",
      "Name: John Schroeder\n",
      "Email: davidreed@hotmail.com\n",
      "Company: Nolan, Bell and Marshall\n",
      "Job Title: Animal nutritionist\n",
      "Date Joined: 2023-06-03\n",
      "Country: Guernsey\n",
      "Salary (USD): 70108.31\n",
      "\n",
      "Name: Madison Morales\n",
      "Email: xsmall@jackson-davis.org\n",
      "Company: Martinez-Torres\n",
      "Job Title: Engineer, electrical\n",
      "Date Joined: 2023-08-09\n",
      "Country: Costa Rica\n",
      "Salary (USD): 94748.99\n",
      "\n",
      "Name: Justin Jones\n",
      "Email: qhopkins@hotmail.com\n",
      "Company: Miller Ltd\n",
      "Job Title: Public librarian\n",
      "Date Joined: 2023-04-30\n",
      "Country: Timor-Leste\n",
      "Salary (USD): 137919.34\n",
      "\n",
      "Name: Adam Rodriguez\n",
      "Email: sotokimberly@yahoo.com\n",
      "Company: Thomas PLC\n",
      "Job Title: Chief Marketing Officer\n",
      "Date Joined: 2024-11-04\n",
      "Country: Belgium\n",
      "Salary (USD): 52139.6\n",
      "\n",
      "Name: Jose Harrington\n",
      "Email: sara97@larson-ellis.com\n",
      "Company: Clark Inc\n",
      "Job Title: Designer, industrial/product\n",
      "Date Joined: 2024-09-07\n",
      "Country: Croatia\n",
      "Salary (USD): 80327.11\n",
      "\n",
      "Name: Megan Smith\n",
      "Email: april08@medina.com\n",
      "Company: Graves-Simpson\n",
      "Job Title: Optometrist\n",
      "Date Joined: 2022-08-16\n",
      "Country: Eritrea\n",
      "Salary (USD): 70877.29\n",
      "\n",
      "Name: Michele Brooks\n",
      "Email: dominguezdeborah@pope.biz\n",
      "Company: Garcia-Meyers\n",
      "Job Title: Interior and spatial designer\n",
      "Date Joined: 2023-11-09\n",
      "Country: Brunei Darussalam\n",
      "Salary (USD): 108354.35\n",
      "\n",
      "Name: William Smith\n",
      "Email: whiteerika@yahoo.com\n",
      "Company: Murphy PLC\n",
      "Job Title: Administrator, charities/voluntary organisations\n",
      "Date Joined: 2024-03-15\n",
      "Country: Montserrat\n",
      "Salary (USD): 94276.17\n",
      "\n",
      "Name: Jack Garcia\n",
      "Email: holly19@hotmail.com\n",
      "Company: Wilson-Daniels\n",
      "Job Title: Ophthalmologist\n",
      "Date Joined: 2023-05-28\n",
      "Country: Congo\n",
      "Salary (USD): 59755.29\n",
      "\n",
      "Name: Nathan Cherry\n",
      "Email: sarah24@hotmail.com\n",
      "Company: Reid-Coleman\n",
      "Job Title: Seismic interpreter\n",
      "Date Joined: 2020-06-20\n",
      "Country: Mozambique\n",
      "Salary (USD): 124402.04\n",
      "\n",
      "Name: Calvin Martinez\n",
      "Email: briannavarro@graham.com\n",
      "Company: Pearson, Evans and Jones\n",
      "Job Title: Therapist, nutritional\n",
      "Date Joined: 2021-04-27\n",
      "Country: United States Virgin Islands\n",
      "Salary (USD): 122658.79\n",
      "\n",
      "Name: James Francis\n",
      "Email: newtonjacob@gmail.com\n",
      "Company: Bryant, Hanna and Wu\n",
      "Job Title: Psychologist, sport and exercise\n",
      "Date Joined: 2022-05-06\n",
      "Country: Heard Island and McDonald Islands\n",
      "Salary (USD): 67073.45\n",
      "\n",
      "Name: Amanda Johnson\n",
      "Email: thomaskimberly@gmail.com\n",
      "Company: Mcdonald PLC\n",
      "Job Title: Dramatherapist\n",
      "Date Joined: 2021-11-29\n",
      "Country: Yemen\n",
      "Salary (USD): 126567.02\n",
      "\n",
      "Name: Benjamin Reeves\n",
      "Email: colemananthony@brown.com\n",
      "Company: Alvarez-Espinoza\n",
      "Job Title: Youth worker\n",
      "Date Joined: 2023-04-03\n",
      "Country: Aruba\n",
      "Salary (USD): 98248.26\n",
      "\n",
      "Name: Joshua Ward\n",
      "Email: rojasscott@yahoo.com\n",
      "Company: King-Barr\n",
      "Job Title: Scientist, research (maths)\n",
      "Date Joined: 2020-09-17\n",
      "Country: Albania\n",
      "Salary (USD): 53950.09\n"
     ]
    }
   ],
   "source": [
    "# To get page_content for all documents in the list\n",
    "all_contents = [doc.page_content for doc in data]\n",
    "\n",
    "# Combine them into a single text\n",
    "combined_text = \"\\n\\n\".join(all_contents)\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results from OpenAI GPT-4.1 ===\n",
      "Here are the requested values:\n",
      "\n",
      "1. **Sum of employee salary**: **$6,269,257.23**\n",
      "\n",
      "2. **Number of employees from Albania**: **2**\n",
      "\n",
      "3. **Country of employee Gabriel Beck**: **Lebanon**\n",
      "\n",
      "=== Results from Anthropic Claude Sonnet ===\n",
      "Based on my analysis of the employee data, here are the values you requested:\n",
      "\n",
      "1. Sum of employee salary: 7,345,656.37\n",
      "2. Number of employees from Albania: 2\n",
      "3. Country of employee Gabriel Beck: Lebanon\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create LLM instances\n",
    "llms = [\n",
    "    (\"OpenAI GPT-4.1\", ChatOpenAI(model=\"gpt-4.1\", api_key=os.getenv(\"OPENAI_API_KEY\"))), \n",
    "    (\"Anthropic Claude Sonnet\", ChatAnthropic(model=\"claude-3-7-sonnet-20250219\", api_key=os.getenv(\"ANTHROPIC_API_KEY\")))\n",
    "]\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a data analyst. Please analyze the following employee data and provide insights:\n",
    "\n",
    "{employee_data}\n",
    "\n",
    "Please provide with the values only:\n",
    "1. Sum of employee salary\n",
    "2. Number of employee from Albania\n",
    "3. Country of employee Gabriel Beck\n",
    "\"\"\")\n",
    "\n",
    "# Loop through each LLM\n",
    "for model_name, llm in llms:\n",
    "    print(f\"\\n=== Results from {model_name} ===\")\n",
    "    \n",
    "    # Create the chain for this specific model\n",
    "    chain = prompt | llm\n",
    "    \n",
    "    # Execute the chain with the combined employee data\n",
    "    try:\n",
    "        response = chain.invoke({\"employee_data\": combined_text})\n",
    "        print(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting result, because as we can see here that they both get the number of employees and the lookup country right but missed on the sum of the salary where the result from Anthropic model closer to the actual result than OpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML document loader (require installation of the unstructured package)\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Splitting External Data for Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document splitting: split document into chunks\n",
    "- Break documents up to fit within an LLMs `context window`\n",
    "\n",
    "- Splitting a document can be done per line, this will be simple to implement but because sentenecs are split between multiple lines and those lines are procesed separately here, key context may be lost. To handle this issue, chunk overlap often implemented where having an extra overlap exist in both chunks help retain context.\n",
    "\n",
    "- There are 2 document splitting methods that will be discussed here they are:\n",
    "    - CharacterTextSplitter\n",
    "    - RecursiveCharacterTextSplitter\n",
    "    - (there are many others, and emerging methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a simple document\n",
    "quote = \"\"\"One machine can do the work of fifty ordinary humans. \\nNo machine can do the work of one extraordinary human.\"\"\"\n",
    "print(len(quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans', 'No machine can do the work of one extraordinary human']\n",
      "[52, 53]\n"
     ]
    }
   ],
   "source": [
    "# CharacterTextSplitter: Splits based on separator first, then evaluate chunk size and chunk overlap\n",
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "\n",
    "chunk_size=24\n",
    "chunk_overlap=3\n",
    "\n",
    "ct_splitter = CharacterTextSplitter(\n",
    "        separator=\".\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = ct_splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method split on separator so it could result to smaller than chunk_size, but may not always succeed. The RecursiveCharacterTextSplitter method is considered more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the', 'work of fifty ordinary', 'humans.', 'No machine can do the', 'work of one', 'extraordinary human.']\n",
      "[22, 22, 7, 21, 11, 20]\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter: Takes a list of separators to split on, and works through the list from left to right, splitting the doc using each separator in turn\n",
    "#                                 and see if these chunks can be combined, while remaining in the chunk_size\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size=24\n",
    "chunk_overlap=3\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice how the lenght of each chunk varies here, the class splits by paragraph first, and found that the chunk size was too big. Likewise for sentences, and then we move on to the third separator (space), splitting words using the space separator and found that words can be combined into chunks while remaining under the chunk size limit.\n",
    "\n",
    "- However some of these chunks are too small to contain meaningful context, but this recursive implementation may works better on larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional: RecursiveCharacterTextSplitter with HTML\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\".\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_documents(data)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. RAG Storage and Retrival Using Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
